{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from collections import deque\n",
    "from operator import itemgetter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['font.family'] = 'IPAexGothic'\n",
    " \n",
    "class DQN(object):\n",
    "    \n",
    "    def __init__(self, env_id, agent_hist_len=4, memory_size=2000,\n",
    "                 replay_start_size=32, gamma=0.99, eps=1.0, eps_min=1e-4,\n",
    "                 final_expl_step=1000, mb_size=32, C=100, n_episodes=400,\n",
    "                 max_steps=500):\n",
    "        \n",
    "        self.env_id = env_id\n",
    "        self.env = gym.make(env_id)\n",
    "        self.path = './data/' + env_id\n",
    "        self.agent_hist_len = agent_hist_len\n",
    "        self.memory_size = memory_size\n",
    "        self.replay_start_size = replay_start_size\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.eps_min = eps_min\n",
    "        self.final_expl_step = final_expl_step\n",
    "        self.eps_decay = (eps-eps_min) / final_expl_step\n",
    "        self.mb_size = mb_size\n",
    "        self.C = C\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        self._init_memory()\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(np.array([t[0] for t in self.memory]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _flatten_deque(d):\n",
    "        return np.array(list(chain(*d)))\n",
    "    \n",
    "    def _get_optimal_action(self, network, agent_hist):\n",
    "        agent_hist_normalized = self.scaler.transform(\n",
    "            self._flatten_deque(agent_hist).reshape(1, -1))\n",
    "        return np.argmax(network.predict(agent_hist_normalized)[0])\n",
    "    \n",
    "    def _get_action(self, agent_hist=None):\n",
    "        if agent_hist is None:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            self.eps = max(self.eps - self.eps_decay, self.eps_min)\n",
    "            if np.random.random() < self.eps:\n",
    "                return self.env.action_space.sample()\n",
    "            else:\n",
    "                return self._get_optimal_action(self.Q, agent_hist)\n",
    "    \n",
    "    def _remember(self, agent_hist, action, reward, new_state, done):\n",
    "        self.memory.append([self._flatten_deque(agent_hist), action, reward,\n",
    "                            new_state if not done else None])\n",
    "    \n",
    "    def _init_memory(self):\n",
    "        print('Initializing replay memory: ', end='')\n",
    "        self.memory = deque(maxlen=self.memory_size)\n",
    "        while True:\n",
    "            state = self.env.reset()\n",
    "            agent_hist = deque(maxlen=self.agent_hist_len)\n",
    "            agent_hist.append(state)\n",
    "            while True:\n",
    "                action = self._get_action(agent_hist=None)\n",
    "                new_state, reward, done, _ = self.env.step(action)\n",
    "                if len(agent_hist) == self.agent_hist_len:\n",
    "                    self._remember(agent_hist, action, reward, new_state, done)\n",
    "                if len(self.memory) == self.replay_start_size:\n",
    "                    print('done')\n",
    "                    return\n",
    "                if done:\n",
    "                    break\n",
    "                state = new_state\n",
    "                agent_hist.append(state)\n",
    "    \n",
    "    def _build_network(self):\n",
    "        nn = Sequential()\n",
    "        nn.add(Dense(20, activation='relu',\n",
    "                     input_dim=(self.agent_hist_len\n",
    "                                * self.env.observation_space.shape[0])))\n",
    "        nn.add(Dense(20, activation='relu'))\n",
    "        nn.add(Dense(10, activation='relu'))\n",
    "        nn.add(Dense(self.env.action_space.n))\n",
    "        nn.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        return nn\n",
    "    \n",
    "    def _clone_network(self, nn):\n",
    "        clone = self._build_network()\n",
    "        clone.set_weights(nn.get_weights())\n",
    "        return clone\n",
    "    \n",
    "    def _get_samples(self):\n",
    "        samples = random.sample(self.memory, self.mb_size)\n",
    "        agent_hists = np.array([s[0] for s in samples])\n",
    "        Y = self.target_Q.predict(self.scaler.transform(agent_hists))\n",
    "        actions = [s[1] for s in samples]\n",
    "        rewards = np.array([s[2] for s in samples])\n",
    "        future_rewards = np.zeros(self.mb_size)\n",
    "        new_states_idx = [i for i, s in enumerate(samples) if s[3] is not None]\n",
    "        new_states = np.array([s[3] for s in itemgetter(*new_states_idx)(samples)])\n",
    "        new_agent_hists = np.hstack(\n",
    "            [agent_hists[new_states_idx, self.env.observation_space.shape[0]:],\n",
    "             new_states])\n",
    "        future_rewards[new_states_idx] = np.max(\n",
    "            self.target_Q.predict(self.scaler.transform(new_agent_hists)), axis=1)\n",
    "        rewards += self.gamma*future_rewards\n",
    "        for i, r in enumerate(Y):\n",
    "            Y[i, actions[i]] = rewards[i]\n",
    "        return agent_hists, Y\n",
    "    \n",
    "    def _replay(self):\n",
    "        agent_hists, Y = self._get_samples()\n",
    "        agent_hists_normalized = self.scaler.transform(agent_hists)\n",
    "        for i in range(self.mb_size):\n",
    "            self.Q.train_on_batch(agent_hists_normalized[i, :].reshape(1, -1),\n",
    "                                  Y[i, :].reshape(1, -1))\n",
    "    \n",
    "    def learn(self, render=False, verbose=True):\n",
    "        \n",
    "        self.Q = self._build_network()\n",
    "        self.target_Q = self._clone_network(self.Q)\n",
    "        \n",
    "        if verbose:\n",
    "            print('Learning target network:')\n",
    "        self.scores = []\n",
    "        for episode in range(self.n_episodes):\n",
    "            state = self.env.reset()\n",
    "            agent_hist = deque(maxlen=self.agent_hist_len)\n",
    "            agent_hist.append(state)\n",
    "            score = 0\n",
    "            for step in range(self.max_steps):\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                if len(agent_hist) < self.agent_hist_len:\n",
    "                    action = self._get_action(agent_hist=None)\n",
    "                else:\n",
    "                    action = self._get_action(agent_hist)\n",
    "                new_state, reward, done, _ = self.env.step(action)\n",
    "                if verbose:\n",
    "                    print('episode: {:4} | step: {:3} | memory: {:6} | \\\n",
    "eps: {:.4f} | action: {} | reward: {: .1f} | best score: {: 6.1f} | \\\n",
    "mean score: {: 6.1f}'.format(\n",
    "                        episode+1, step+1, len(self.memory), self.eps, action, reward,\n",
    "                        max(self.scores) if len(self.scores) != 0 else np.nan,\n",
    "                        np.mean(self.scores) if len(self.scores) != 0 else np.nan),\n",
    "                        end='\\r')                        \n",
    "                score += reward\n",
    "                if len(agent_hist) == self.agent_hist_len:\n",
    "                    self._remember(agent_hist, action, reward, new_state, done)\n",
    "                    self._replay()\n",
    "                if step % self.C == 0:\n",
    "                    self.target_Q = self._clone_network(self.Q)\n",
    "                if done:\n",
    "                    self.scores.append(score)\n",
    "                    break\n",
    "                state = new_state\n",
    "                agent_hist.append(state)\n",
    "        \n",
    "        self.target_Q.save(self.path + '_model.h5')\n",
    "        with open(self.path + '_scores.pkl', 'wb') as f:\n",
    "            pickle.dump(self.scores, f)\n",
    "    \n",
    "    def plot_training_scores(self):\n",
    "        with open(self.path + '_scores.pkl', 'rb') as f:\n",
    "            scores = pd.Series(pickle.load(f))\n",
    "        avg_scores = scores.cumsum() / (scores.index + 1)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        n_scores = len(scores)\n",
    "        plt.plot(range(n_scores), scores, color='gray', linewidth=1)\n",
    "        plt.plot(range(n_scores), avg_scores, label='??')\n",
    "        plt.legend()\n",
    "        plt.xlabel('???????')\n",
    "        plt.ylabel('???')\n",
    "        plt.title(self.env_id)\n",
    "        plt.margins(0.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def run(self, render=True):\n",
    "        \n",
    "        fname = self.path + '_model.h5'\n",
    "        if os.path.exists(fname):\n",
    "            self.target_Q = load_model(fname)\n",
    "        else:\n",
    "            print('Q-network not found. Start learning.')\n",
    "            self.learn()\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        agent_hist = deque(maxlen=self.agent_hist_len)\n",
    "        agent_hist.extend([state]*self.agent_hist_len)\n",
    "        score = 0\n",
    "        while True:\n",
    "            if render:\n",
    "                self.env.render()\n",
    "            action = self._get_optimal_action(self.target_Q, agent_hist)\n",
    "            new_state, reward, done, _ = self.env.step(action)\n",
    "            score += reward\n",
    "            if done:\n",
    "                print('{} score: {}'.format(self.env_id, score))\n",
    "                return\n",
    "            state = new_state\n",
    "            agent_hist.append(state)\n",
    "            time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Initializing replay memory: done\n",
      "Learning target network:\n",
      "episode:  400 | step: 104 | memory:   2000 | eps: 0.0001 | action: 0 | reward:  1.0 | best score:  500.0 | mean score:  195.6\r"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = './data/CartPole-v1_model.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1c386f5c082b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CartPole-v1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-33d129b10510>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, render, verbose)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0magent_hist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_Q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_scores.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py3envtf2/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py3envtf2/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py3envtf2/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py3envtf2/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = './data/CartPole-v1_model.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)"
     ]
    }
   ],
   "source": [
    "dqn = DQN('CartPole-v1')\n",
    "dqn.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
