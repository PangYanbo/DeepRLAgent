{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "from keras.callbacks import History\n",
    "\n",
    "from rl.callbacks import (\n",
    "    CallbackList,\n",
    "    TestLogger,\n",
    "    TrainEpisodeLogger,\n",
    "    TrainIntervalLogger,\n",
    "    Visualizer\n",
    ")\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "    \"\"\"Abstract base class for all implemented agents.\n",
    "    Each agent interacts with the environment (as defined by the `Env` class) by first observing the\n",
    "    state of the environment. Based on this observation the agent changes the environment by performing\n",
    "    an action.\n",
    "    Do not use this abstract base class directly but instead use one of the concrete agents implemented.\n",
    "    Each agent realizes a reinforcement learning algorithm. Since all agents conform to the same\n",
    "    interface, you can use them interchangeably.\n",
    "    To implement your own agent, you have to implement the following methods:\n",
    "    - `forward`\n",
    "    - `backward`\n",
    "    - `compile`\n",
    "    - `load_weights`\n",
    "    - `save_weights`\n",
    "    - `layers`\n",
    "    # Arguments\n",
    "        processor (`Processor` instance): See [Processor](#processor) for details.\n",
    "    \"\"\"\n",
    "    def __init__(self, processor=None):\n",
    "        self.processor = processor\n",
    "        self.training = False\n",
    "        self.step = 0\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Configuration of the agent for serialization.\n",
    "        # Returns\n",
    "            Dictionnary with agent configuration\n",
    "        \"\"\"\n",
    "        return {}\n",
    "\n",
    "    def fit(self, env, nb_steps,graph, action_repetition=1, callbacks=None, verbose=1,\n",
    "            visualize=False, nb_max_start_steps=0, start_step_policy=None, log_interval=10000,\n",
    "            nb_max_episode_steps=None):\n",
    "        \"\"\"Trains the agent on the given environment.\n",
    "        # Arguments\n",
    "            env: (`Env` instance): Environment that the agent interacts with. See [Env](#env) for details.\n",
    "            nb_steps (integer): Number of training steps to be performed.\n",
    "            action_repetition (integer): Number of times the agent repeats the same action without\n",
    "                observing the environment again. Setting this to a value > 1 can be useful\n",
    "                if a single action only has a very small effect on the environment.\n",
    "            callbacks (list of `keras.callbacks.Callback` or `rl.callbacks.Callback` instances):\n",
    "                List of callbacks to apply during training. See [callbacks](/callbacks) for details.\n",
    "            verbose (integer): 0 for no logging, 1 for interval logging (compare `log_interval`), 2 for episode logging\n",
    "            visualize (boolean): If `True`, the environment is visualized during training. However,\n",
    "                this is likely going to slow down training significantly and is thus intended to be\n",
    "                a debugging instrument.\n",
    "            nb_max_start_steps (integer): Number of maximum steps that the agent performs at the beginning\n",
    "                of each episode using `start_step_policy`. Notice that this is an upper limit since\n",
    "                the exact number of steps to be performed is sampled uniformly from [0, max_start_steps]\n",
    "                at the beginning of each episode.\n",
    "            start_step_policy (`lambda observation: action`): The policy\n",
    "                to follow if `nb_max_start_steps` > 0. If set to `None`, a random action is performed.\n",
    "            log_interval (integer): If `verbose` = 1, the number of steps that are considered to be an interval.\n",
    "            nb_max_episode_steps (integer): Number of steps per episode that the agent performs before\n",
    "                automatically resetting the environment. Set to `None` if each episode should run\n",
    "                (potentially indefinitely) until the environment signals a terminal state.\n",
    "        # Returns\n",
    "            A `keras.callbacks.History` instance that recorded the entire training process.\n",
    "        \"\"\"\n",
    "        if not self.compiled:\n",
    "            raise RuntimeError('Your tried to fit your agent but it hasn\\'t been compiled yet. Please call `compile()` before `fit()`.')\n",
    "        if action_repetition < 1:\n",
    "            raise ValueError('action_repetition must be >= 1, is {}'.format(action_repetition))\n",
    "\n",
    "        self.training = True\n",
    "\n",
    "        callbacks = [] if not callbacks else callbacks[:]\n",
    "\n",
    "        if verbose == 1:\n",
    "            callbacks += [TrainIntervalLogger(interval=log_interval)]\n",
    "        elif verbose > 1:\n",
    "            callbacks += [TrainEpisodeLogger()]\n",
    "        if visualize:\n",
    "            callbacks += [Visualizer()]\n",
    "        history = History()\n",
    "        callbacks += [history]\n",
    "        callbacks = CallbackList(callbacks)\n",
    "        if hasattr(callbacks, 'set_model'):\n",
    "            callbacks.set_model(self)\n",
    "        else:\n",
    "            callbacks._set_model(self)\n",
    "        callbacks._set_env(env)\n",
    "        params = {\n",
    "            'nb_steps': nb_steps,\n",
    "        }\n",
    "        if hasattr(callbacks, 'set_params'):\n",
    "            callbacks.set_params(params)\n",
    "        else:\n",
    "            callbacks._set_params(params)\n",
    "        self._on_train_begin()\n",
    "        callbacks.on_train_begin()\n",
    "\n",
    "        episode = np.int16(0)\n",
    "        self.step = np.int16(0)\n",
    "        observation = None\n",
    "        episode_reward = None\n",
    "        episode_step = None\n",
    "        did_abort = False\n",
    "        try:\n",
    "            while self.step < nb_steps:\n",
    "                if observation is None:  # start of a new episode\n",
    "                    callbacks.on_episode_begin(episode)\n",
    "                    episode_step = np.int16(0)\n",
    "                    episode_reward = np.float32(0)\n",
    "\n",
    "                    # Obtain the initial observation by resetting the environment.\n",
    "                    self.reset_states()\n",
    "                    observation = deepcopy(env.reset())\n",
    "                    if self.processor is not None:\n",
    "                        observation = self.processor.process_observation(observation)\n",
    "                    assert observation is not None\n",
    "\n",
    "                    # Perform random starts at beginning of episode and do not record them into the experience.\n",
    "                    # This slightly changes the start position between games.\n",
    "                    nb_random_start_steps = 0 if nb_max_start_steps == 0 else np.random.randint(nb_max_start_steps)\n",
    "                    for _ in range(nb_random_start_steps):\n",
    "                        if start_step_policy is None:\n",
    "                            action = env.action_space.sample()\n",
    "                        else:\n",
    "                            action = start_step_policy(observation)    ###ここか？###\n",
    "                        if self.processor is not None:\n",
    "                            action = self.processor.process_action(action)\n",
    "                        callbacks.on_action_begin(action)\n",
    "                        observation, reward, done, info = env.step(action)\n",
    "                        observation = deepcopy(observation)\n",
    "                        if self.processor is not None:\n",
    "                            observation, reward, done, info = self.processor.process_step(observation, reward, done, info)\n",
    "                        callbacks.on_action_end(action)\n",
    "                        if done:\n",
    "                            warnings.warn('Env ended before {} random steps could be performed at the start. You should probably lower the `nb_max_start_steps` parameter.'.format(nb_random_start_steps))\n",
    "                            observation = deepcopy(env.reset())\n",
    "                            if self.processor is not None:\n",
    "                                observation = self.processor.process_observation(observation)\n",
    "                            break\n",
    "\n",
    "                # At this point, we expect to be fully initialized.\n",
    "                assert episode_reward is not None\n",
    "                assert episode_step is not None\n",
    "                assert observation is not None\n",
    "\n",
    "                # Run a single step.\n",
    "                callbacks.on_step_begin(episode_step)\n",
    "                # This is were all of the work happens. We first perceive and compute the action\n",
    "                # (forward step) and then use the reward to improve (backward step).\n",
    "                graph = env.graph_set.graph #最新のグラフでfilter_nodeを構成するように、とりあえずself.forwardの前にgraph更新を挟む\n",
    "                action = self.forward(observation,graph) ###ここか!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!###\n",
    "                if self.processor is not None:\n",
    "                    action = self.processor.process_action(action)\n",
    "                reward = np.float32(0)\n",
    "                accumulated_info = {}\n",
    "                done = False\n",
    "                for _ in range(action_repetition):\n",
    "                    callbacks.on_action_begin(action)\n",
    "                    observation, r, done, info = env.step(action)\n",
    "                    observation = deepcopy(observation)\n",
    "                    if self.processor is not None:\n",
    "                        observation, r, done, info = self.processor.process_step(observation, r, done, info)\n",
    "                    for key, value in info.items():\n",
    "                        if not np.isreal(value):\n",
    "                            continue\n",
    "                        if key not in accumulated_info:\n",
    "                            accumulated_info[key] = np.zeros_like(value)\n",
    "                        accumulated_info[key] += value\n",
    "                    callbacks.on_action_end(action)\n",
    "                    reward += r\n",
    "                    if done:\n",
    "                        break\n",
    "                if nb_max_episode_steps and episode_step >= nb_max_episode_steps - 1:\n",
    "                    # Force a terminal state.\n",
    "                    done = True\n",
    "                metrics = self.backward(env.graph_set.graph,reward, terminal=done)\n",
    "                episode_reward += reward\n",
    "\n",
    "                step_logs = {\n",
    "                    'action': action,\n",
    "                    'observation': observation,\n",
    "                    'reward': reward,\n",
    "                    'metrics': metrics,\n",
    "                    'episode': episode,\n",
    "                    'info': accumulated_info,\n",
    "                }\n",
    "                callbacks.on_step_end(episode_step, step_logs)\n",
    "                episode_step += 1\n",
    "                self.step += 1\n",
    "\n",
    "                if done:\n",
    "                    # We are in a terminal state but the agent hasn't yet seen it. We therefore\n",
    "                    # perform one more forward-backward call and simply ignore the action before\n",
    "                    # resetting the environment. We need to pass in `terminal=False` here since\n",
    "                    # the *next* state, that is the state of the newly reset environment, is\n",
    "                    # always non-terminal by convention.\n",
    "                    graph = env.graph_set.graph #最新のグラフでfilter_nodeを構成するように、とりあえずself.forwardの前にgraph更新を挟む\n",
    "                    self.forward(observation,graph) ###ここは！！！！！！！！！！！！！！！！！！！！！！！###\n",
    "                    self.backward(env.graph_set.graph,0., terminal=False)\n",
    "\n",
    "                    # This episode is finished, report and reset.\n",
    "                    episode_logs = {\n",
    "                        'episode_reward': episode_reward,\n",
    "                        'nb_episode_steps': episode_step,\n",
    "                        'nb_steps': self.step,\n",
    "                    }\n",
    "                    callbacks.on_episode_end(episode, episode_logs)\n",
    "\n",
    "                    episode += 1\n",
    "                    observation = None\n",
    "                    episode_step = None\n",
    "                    episode_reward = None\n",
    "        except KeyboardInterrupt:\n",
    "            # We catch keyboard interrupts here so that training can be be safely aborted.\n",
    "            # This is so common that we've built this right into this function, which ensures that\n",
    "            # the `on_train_end` method is properly called.\n",
    "            did_abort = True\n",
    "        callbacks.on_train_end(logs={'did_abort': did_abort})\n",
    "        self._on_train_end()\n",
    "\n",
    "        return history\n",
    "\n",
    "    def test(self, env, graph, nb_episodes=1, action_repetition=1, callbacks=None, visualize=True,\n",
    "             nb_max_episode_steps=None, nb_max_start_steps=0, start_step_policy=None, verbose=2):\n",
    "        \"\"\"Callback that is called before training begins.\n",
    "        # Arguments\n",
    "            env: (`Env` instance): Environment that the agent interacts with. See [Env](#env) for details.\n",
    "            nb_episodes (integer): Number of episodes to perform.\n",
    "            action_repetition (integer): Number of times the agent repeats the same action without\n",
    "                observing the environment again. Setting this to a value > 1 can be useful\n",
    "                if a single action only has a very small effect on the environment.\n",
    "            callbacks (list of `keras.callbacks.Callback` or `rl.callbacks.Callback` instances):\n",
    "                List of callbacks to apply during training. See [callbacks](/callbacks) for details.\n",
    "            verbose (integer): 0 for no logging, 1 for interval logging (compare `log_interval`), 2 for episode logging\n",
    "            visualize (boolean): If `True`, the environment is visualized during training. However,\n",
    "                this is likely going to slow down training significantly and is thus intended to be\n",
    "                a debugging instrument.\n",
    "            nb_max_start_steps (integer): Number of maximum steps that the agent performs at the beginning\n",
    "                of each episode using `start_step_policy`. Notice that this is an upper limit since\n",
    "                the exact number of steps to be performed is sampled uniformly from [0, max_start_steps]\n",
    "                at the beginning of each episode.\n",
    "            start_step_policy (`lambda observation: action`): The policy\n",
    "                to follow if `nb_max_start_steps` > 0. If set to `None`, a random action is performed.\n",
    "            log_interval (integer): If `verbose` = 1, the number of steps that are considered to be an interval.\n",
    "            nb_max_episode_steps (integer): Number of steps per episode that the agent performs before\n",
    "                automatically resetting the environment. Set to `None` if each episode should run\n",
    "                (potentially indefinitely) until the environment signals a terminal state.\n",
    "        # Returns\n",
    "            A `keras.callbacks.History` instance that recorded the entire training process.\n",
    "        \"\"\"\n",
    "        if not self.compiled:\n",
    "            raise RuntimeError('Your tried to test your agent but it hasn\\'t been compiled yet. Please call `compile()` before `test()`.')\n",
    "        if action_repetition < 1:\n",
    "            raise ValueError('action_repetition must be >= 1, is {}'.format(action_repetition))\n",
    "\n",
    "        self.training = False\n",
    "        self.step = 0\n",
    "\n",
    "        callbacks = [] if not callbacks else callbacks[:]\n",
    "\n",
    "        if verbose >= 1:\n",
    "            callbacks += [TestLogger()]\n",
    "        if visualize:\n",
    "            callbacks += [Visualizer()]\n",
    "        history = History()\n",
    "        callbacks += [history]\n",
    "        callbacks = CallbackList(callbacks)\n",
    "        if hasattr(callbacks, 'set_model'):\n",
    "            callbacks.set_model(self)\n",
    "        else:\n",
    "            callbacks._set_model(self)\n",
    "        callbacks._set_env(env)\n",
    "        params = {\n",
    "            'nb_episodes': nb_episodes,\n",
    "        }\n",
    "        if hasattr(callbacks, 'set_params'):\n",
    "            callbacks.set_params(params)\n",
    "        else:\n",
    "            callbacks._set_params(params)\n",
    "\n",
    "        self._on_test_begin()\n",
    "        callbacks.on_train_begin()\n",
    "        for episode in range(nb_episodes):\n",
    "            callbacks.on_episode_begin(episode)\n",
    "            episode_reward = 0.\n",
    "            episode_step = 0\n",
    "\n",
    "            # Obtain the initial observation by resetting the environment.\n",
    "            self.reset_states()\n",
    "            observation = deepcopy(env.reset())\n",
    "            if self.processor is not None:\n",
    "                observation = self.processor.process_observation(observation)\n",
    "            assert observation is not None\n",
    "            \n",
    "            # the folowing steps are omitted because we already applied an accident to the graph (env)\n",
    "            # out of the scope (in main function)\n",
    "            #and this is test function and we dont want randomness for the accuracy\n",
    "            #however, now in the standard state, the program doesn't execute this part\n",
    "            \n",
    "            # Perform random starts at beginning of episode and do not record them into the experience.\n",
    "            # This slightly changes the start position between games.\n",
    "            nb_random_start_steps = 0 if nb_max_start_steps == 0 else np.random.randint(nb_max_start_steps)\n",
    "            for _ in range(nb_random_start_steps):\n",
    "                print(\"WARINING: random actio occurs\")\n",
    "                if start_step_policy is None:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = start_step_policy(observation)\n",
    "                if self.processor is not None:\n",
    "                    action = self.processor.process_action(action)\n",
    "                callbacks.on_action_begin(action)\n",
    "                observation, r, done, info = env.step(action) #\n",
    "                observation = deepcopy(observation)\n",
    "                if self.processor is not None:\n",
    "                    observation, r, done, info = self.processor.process_step(observation, r, done, info)\n",
    "                callbacks.on_action_end(action)\n",
    "                if done:\n",
    "                    warnings.warn('Env ended before {} random steps could be performed at the start. You should probably lower the `nb_max_start_steps` parameter.'.format(nb_random_start_steps))\n",
    "                    observation = deepcopy(env.reset())\n",
    "                    if self.processor is not None:\n",
    "                        observation = self.processor.process_observation(observation)\n",
    "                    break\n",
    "            \n",
    "            \n",
    "            # Run the episode until we're done.\n",
    "            done = False\n",
    "            while not done:\n",
    "                callbacks.on_step_begin(episode_step)\n",
    "                graph = env.graph_set.graph \n",
    "                #最新のグラフでfilter_nodeを構成するように、とりあえずself.forwardの前にgraph更新を挟む\n",
    "                action = self.forward(observation,graph) ###ここは!!###  \n",
    "                #print(\"actioned!\")\n",
    "                if self.processor is not None:\n",
    "                    action = self.processor.process_action(action)\n",
    "                reward = 0.\n",
    "                accumulated_info = {}\n",
    "                #print(\"acciton_repetition\", action_repetition)\n",
    "                for _ in range(action_repetition):\n",
    "                    callbacks.on_action_begin(action)\n",
    "                    observation, r, d, info = env.step(action)\n",
    "                    observation = deepcopy(observation)\n",
    "                    if self.processor is not None:\n",
    "                        observation, r, d, info = self.processor.process_step(observation, r, d, info)\n",
    "                    callbacks.on_action_end(action)\n",
    "                    reward += r\n",
    "                    for key, value in info.items():\n",
    "                        if not np.isreal(value):\n",
    "                            continue\n",
    "                        if key not in accumulated_info:\n",
    "                            accumulated_info[key] = np.zeros_like(value)\n",
    "                        accumulated_info[key] += value\n",
    "                    if d:\n",
    "                        done = True\n",
    "                        break\n",
    "                if nb_max_episode_steps and episode_step >= nb_max_episode_steps - 1:\n",
    "                    done = True\n",
    "                self.backward(env.graph_set.graph,reward, terminal=done)\n",
    "                episode_reward += reward\n",
    "\n",
    "                step_logs = {\n",
    "                    'action': action,\n",
    "                    'observation': observation,\n",
    "                    'reward': reward,\n",
    "                    'episode': episode,\n",
    "                    'info': accumulated_info,\n",
    "                }\n",
    "                print(step_logs['action']) #this row is added\n",
    "                callbacks.on_step_end(episode_step, step_logs)\n",
    "                episode_step += 1\n",
    "                self.step += 1\n",
    "\n",
    "            # We are in a terminal state but the agent hasn't yet seen it. We therefore\n",
    "            # perform one more forward-backward call and simply ignore the action before\n",
    "            # resetting the environment. We need to pass in `terminal=False` here since\n",
    "            # the *next* state, that is the state of the newly reset environment, is\n",
    "            # always non-terminal by convention.\n",
    "            graph = env.graph_set.graph #最新のグラフでfilter_nodeを構成するように、とりあえずself.forwardの前にgraph更新を挟む\n",
    "            self.forward(observation,graph)  ###ここは！！####\n",
    "            self.backward(env.graph_set.graph,0., terminal=False)\n",
    "\n",
    "            # Report end of episode.\n",
    "            episode_logs = {\n",
    "                'episode_reward': episode_reward,\n",
    "                'nb_steps': episode_step,\n",
    "            }\n",
    "            callbacks.on_episode_end(episode, episode_logs)\n",
    "        callbacks.on_train_end()\n",
    "        self._on_test_end()\n",
    "\n",
    "        return history\n",
    "\n",
    "    def reset_states(self):\n",
    "        \"\"\"Resets all internally kept states after an episode is completed.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, observation,graph):  ###まあ念の為一応graphを引数に追加しておきました。###\n",
    "        \"\"\"Takes the an observation from the environment and returns the action to be taken next.\n",
    "        If the policy is implemented by a neural network, this corresponds to a forward (inference) pass.\n",
    "        # Argument\n",
    "            observation (object): The current observation from the environment.\n",
    "        # Returns\n",
    "            The next action to be executed in the environment.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward(self, graph,reward, terminal):\n",
    "        \"\"\"Updates the agent after having executed the action returned by `forward`.\n",
    "        If the policy is implemented by a neural network, this corresponds to a weight update using back-prop.\n",
    "        # Argument\n",
    "            reward (float): The observed reward after executing the action returned by `forward`.\n",
    "            terminal (boolean): `True` if the new state of the environment is terminal.\n",
    "        # Returns\n",
    "            List of metrics values\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def compile(self, optimizer, metrics=[]):\n",
    "        \"\"\"Compiles an agent and the underlaying models to be used for training and testing.\n",
    "        # Arguments\n",
    "            optimizer (`keras.optimizers.Optimizer` instance): The optimizer to be used during training.\n",
    "            metrics (list of functions `lambda y_true, y_pred: metric`): The metrics to run during training.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def load_weights(self, filepath):\n",
    "        \"\"\"Loads the weights of an agent from an HDF5 file.\n",
    "        # Arguments\n",
    "            filepath (str): The path to the HDF5 file.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def save_weights(self, filepath, overwrite=False):\n",
    "        \"\"\"Saves the weights of an agent as an HDF5 file.\n",
    "        # Arguments\n",
    "            filepath (str): The path to where the weights should be saved.\n",
    "            overwrite (boolean): If `False` and `filepath` already exists, raises an error.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        \"\"\"Returns all layers of the underlying model(s).\n",
    "        If the concrete implementation uses multiple internal models,\n",
    "        this method returns them in a concatenated list.\n",
    "        # Returns\n",
    "            A list of the model's layers\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def metrics_names(self):\n",
    "        \"\"\"The human-readable names of the agent's metrics. Must return as many names as there\n",
    "        are metrics (see also `compile`).\n",
    "        # Returns\n",
    "            A list of metric's names (string)\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    def _on_train_begin(self):\n",
    "        \"\"\"Callback that is called before training begins.\"\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_train_end(self):\n",
    "        \"\"\"Callback that is called after training ends.\"\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_test_begin(self):\n",
    "        \"\"\"Callback that is called before testing begins.\"\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_test_end(self):\n",
    "        \"\"\"Callback that is called after testing ends.\"\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class Processor(object):\n",
    "    \"\"\"Abstract base class for implementing processors.\n",
    "    A processor acts as a coupling mechanism between an `Agent` and its `Env`. This can\n",
    "    be necessary if your agent has different requirements with respect to the form of the\n",
    "    observations, actions, and rewards of the environment. By implementing a custom processor,\n",
    "    you can effectively translate between the two without having to change the underlaying\n",
    "    implementation of the agent or environment.\n",
    "    Do not use this abstract base class directly but instead use one of the concrete implementations\n",
    "    or write your own.\n",
    "    \"\"\"\n",
    "\n",
    "    def process_step(self, observation, reward, done, info):\n",
    "        \"\"\"Processes an entire step by applying the processor to the observation, reward, and info arguments.\n",
    "        # Arguments\n",
    "            observation (object): An observation as obtained by the environment.\n",
    "            reward (float): A reward as obtained by the environment.\n",
    "            done (boolean): `True` if the environment is in a terminal state, `False` otherwise.\n",
    "            info (dict): The debug info dictionary as obtained by the environment.\n",
    "        # Returns\n",
    "            The tupel (observation, reward, done, reward) with with all elements after being processed.\n",
    "        \"\"\"\n",
    "        observation = self.process_observation(observation)\n",
    "        reward = self.process_reward(reward)\n",
    "        info = self.process_info(info)\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def process_observation(self, observation):\n",
    "        \"\"\"Processes the observation as obtained from the environment for use in an agent and\n",
    "        returns it.\n",
    "        # Arguments\n",
    "            observation (object): An observation as obtained by the environment\n",
    "        # Returns\n",
    "            Observation obtained by the environment processed\n",
    "        \"\"\"\n",
    "        return observation\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        \"\"\"Processes the reward as obtained from the environment for use in an agent and\n",
    "        returns it.\n",
    "        # Arguments\n",
    "            reward (float): A reward as obtained by the environment\n",
    "        # Returns\n",
    "            Reward obtained by the environment processed\n",
    "        \"\"\"\n",
    "        return reward\n",
    "\n",
    "    def process_info(self, info):\n",
    "        \"\"\"Processes the info as obtained from the environment for use in an agent and\n",
    "        returns it.\n",
    "        # Arguments\n",
    "            info (dict): An info as obtained by the environment\n",
    "        # Returns\n",
    "            Info obtained by the environment processed\n",
    "        \"\"\"\n",
    "        return info\n",
    "\n",
    "    def process_action(self, action):\n",
    "        \"\"\"Processes an action predicted by an agent but before execution in an environment.\n",
    "        # Arguments\n",
    "            action (int): Action given to the environment\n",
    "        # Returns\n",
    "            Processed action given to the environment\n",
    "        \"\"\"\n",
    "        return action\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        \"\"\"Processes an entire batch of states and returns it.\n",
    "        # Arguments\n",
    "            batch (list): List of states\n",
    "        # Returns\n",
    "            Processed list of states\n",
    "        \"\"\"\n",
    "        return batch\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        \"\"\"The metrics of the processor, which will be reported during training.\n",
    "        # Returns\n",
    "            List of `lambda y_true, y_pred: metric` functions.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def metrics_names(self):\n",
    "        \"\"\"The human-readable names of the agent's metrics. Must return as many names as there\n",
    "        are metrics (see also `compile`).\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "\n",
    "# Note: the API of the `Env` and `Space` classes are taken from the OpenAI Gym implementation.\n",
    "# https://github.com/openai/gym/blob/master/gym/core.py\n",
    "\n",
    "\n",
    "class Env(object):\n",
    "    \"\"\"The abstract environment class that is used by all agents. This class has the exact\n",
    "    same API that OpenAI Gym uses so that integrating with it is trivial. In contrast to the\n",
    "    OpenAI Gym implementation, this class only defines the abstract methods without any actual\n",
    "    implementation.\n",
    "    To implement your own environment, you need to define the following methods:\n",
    "    - `step`\n",
    "    - `reset`\n",
    "    - `render`\n",
    "    - `close`\n",
    "    Refer to the [Gym documentation](https://gym.openai.com/docs/#environments).\n",
    "    \"\"\"\n",
    "    reward_range = (-np.inf, np.inf)\n",
    "    action_space = None\n",
    "    observation_space = None\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Run one timestep of the environment's dynamics.\n",
    "        Accepts an action and returns a tuple (observation, reward, done, info).\n",
    "        # Arguments\n",
    "            action (object): An action provided by the environment.\n",
    "        # Returns\n",
    "            observation (object): Agent's observation of the current environment.\n",
    "            reward (float) : Amount of reward returned after previous action.\n",
    "            done (boolean): Whether the episode has ended, in which case further step() calls will return undefined results.\n",
    "            info (dict): Contains auxiliary diagnostic information (helpful for debugging, and sometimes learning).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the state of the environment and returns an initial observation.\n",
    "        # Returns\n",
    "            observation (object): The initial observation of the space. Initial reward is assumed to be 0.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        \"\"\"Renders the environment.\n",
    "        The set of supported modes varies per environment. (And some\n",
    "        environments do not support rendering at all.)\n",
    "        # Arguments\n",
    "            mode (str): The mode to render with.\n",
    "            close (bool): Close all open renderings.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Override in your subclass to perform any necessary cleanup.\n",
    "        Environments will automatically close() themselves when\n",
    "        garbage collected or when the program exits.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"Sets the seed for this env's random number generator(s).\n",
    "        # Returns\n",
    "            Returns the list of seeds used in this env's random number generators\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def configure(self, *args, **kwargs):\n",
    "        \"\"\"Provides runtime configuration to the environment.\n",
    "        This configuration should consist of data that tells your\n",
    "        environment how to run (such as an address of a remote server,\n",
    "        or path to your ImageNet data). It should not affect the\n",
    "        semantics of the environment.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "    def __str__(self):\n",
    "        return '<{} instance>'.format(type(self).__name__)\n",
    "\n",
    "\n",
    "class Space(object):\n",
    "    \"\"\"Abstract model for a space that is used for the state and action spaces. This class has the\n",
    "    exact same API that OpenAI Gym uses so that integrating with it is trivial.\n",
    "    Please refer to [Gym Documentation](https://gym.openai.com/docs/#spaces)\n",
    "    \"\"\"\n",
    "\n",
    "    def sample(self, seed=None):\n",
    "        \"\"\"Uniformly randomly sample a random element of this space.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def contains(self, x):\n",
    "        \"\"\"Return boolean specifying if x is a valid member of this space\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "#import numpy as np\n",
    "from copy import copy\n",
    "from rl.util import *\n",
    "\n",
    "\n",
    "class Policy(object):\n",
    "    \"\"\"Abstract base class for all implemented policies.\n",
    "    Each policy helps with selection of action to take on an environment.\n",
    "    Do not use this abstract base class directly but instead use one of the concrete policies implemented.\n",
    "    To implement your own policy, you have to implement the following methods:\n",
    "    - `select_action`\n",
    "    # Arguments\n",
    "        agent (rl.core.Agent): Agent used\n",
    "    \"\"\"\n",
    "    def _set_agent(self, agent):\n",
    "        self.agent = agent\n",
    "\n",
    "    @property\n",
    "    def metrics_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return []\n",
    "\n",
    "    def select_action(self, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Return configuration of the policy\n",
    "        # Returns\n",
    "            Configuration as dict\n",
    "        \"\"\"\n",
    "        return {}\n",
    "\n",
    "\n",
    "class EpsGreedyQPolicy(Policy):\n",
    "    \"\"\"Implement the epsilon greedy policy\n",
    "    \n",
    "    Eps Greedy policy either:\n",
    "    \n",
    "    - takes a random action with probability epsilon\n",
    "    - takes current best action with prob (1 - epsilon)\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=.1):\n",
    "        super(EpsGreedyQPolicy, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def select_action(self, q_values,graph): #node_filterを追加しています。\n",
    "        \"\"\"Return the selected action\n",
    "        # Arguments\n",
    "            q_values (np.ndarray): List of the estimations of Q for each action\n",
    "        # Returns\n",
    "            Selection action\n",
    "        \"\"\"\n",
    "        assert q_values.ndim == 1\n",
    "        nb_actions = q_values.shape[0]\n",
    "\n",
    "        if np.random.uniform() < self.eps:\n",
    "            v_num = len(graph.nodes)\n",
    "            node_filter = readed.filter_node(graph,v_num)\n",
    "            index_filtered = [i for i,j in zip(range(len(node_filter)), node_filter) if j ==1]\n",
    "            #print(\"index\")\n",
    "            #print(index_filtered)\n",
    "            action = np.random.choice(index_filtered)\n",
    "            #action = np.random.random_integers(0, nb_actions-1)\n",
    "        else:\n",
    "            #ここでfilter_nodeかける。\n",
    "            v_num = len(graph.nodes)\n",
    "            node_filter = readed.filter_node(graph,v_num)\n",
    "            #print(\"len:qval\",len(q_values))\n",
    "            #print(\"len:filter\",len(node_filter))\n",
    "            MINUS_INF = -999999\n",
    "            masked_q_values = copy(q_values)\n",
    "            for i in range(len(q_values)):\n",
    "                if(node_filter[i]!=1):\n",
    "                    masked_q_values[i]= MINUS_INF\n",
    "            action = np.argmax(masked_q_values)\n",
    "            '''\n",
    "            print(\"action\",action)\n",
    "            print(\"filter\")\n",
    "            print(node_filter)\n",
    "            print(\"node:q_val\")\n",
    "            print(q_values[np.argmax(q_values)])\n",
    "            print(\"qvaltable\")\n",
    "            print(q_values)\n",
    "            input()\n",
    "            '''\n",
    "        return action\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Return configurations of EpsGreedyPolicy\n",
    "        # Returns\n",
    "            Dict of config\n",
    "        \"\"\"\n",
    "        config = super(EpsGreedyQPolicy, self).get_config()\n",
    "        config['eps'] = self.eps\n",
    "        return config\n",
    "\n",
    "\n",
    "class GreedyQPolicy(Policy):\n",
    "    \"\"\"Implement the greedy policy\n",
    "    Greedy policy returns the current best action according to q_values\n",
    "    \"\"\"\n",
    "    def select_action(self, q_values,graph):\n",
    "        \"\"\"Return the selected action\n",
    "        # Arguments\n",
    "            q_values (np.ndarray): List of the estimations of Q for each action\n",
    "        # Returns\n",
    "            Selection action\n",
    "        \"\"\"\n",
    "        assert q_values.ndim == 1\n",
    "        action = np.argmax(q_values)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/keras-rl/keras-rl/blob/master/rl/agents/dqn.py\n",
    "\n",
    "#filter_nodeを作って、グリーディに取るときの選択肢subsetを有効なもののみにする！\n",
    "\n",
    "#from __future__ import division\n",
    "#import warnings\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Lambda, Input, Layer, Dense\n",
    "\n",
    "#from rl.core import Agent\n",
    "#from rl.policy import EpsGreedyQPolicy, GreedyQPolicy\n",
    "from rl.util import *\n",
    "\n",
    "#from rl.agents.dqn import AbstractDQNAgent\n",
    "\n",
    "\n",
    "def mean_q(y_true, y_pred):\n",
    "    return K.mean(K.max(y_pred, axis=-1))\n",
    "\n",
    "class AbstractDQNAgent(Agent):\n",
    "    \"\"\"Write me\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_actions, memory, gamma=.99, batch_size=32, nb_steps_warmup=1000,\n",
    "                 train_interval=1, memory_interval=1, target_model_update=10000,\n",
    "                 delta_range=None, delta_clip=np.inf, custom_model_objects={}, **kwargs):\n",
    "        super(AbstractDQNAgent, self).__init__(**kwargs)\n",
    "\n",
    "        # Soft vs hard target model updates.\n",
    "        if target_model_update < 0:\n",
    "            raise ValueError('`target_model_update` must be >= 0.')\n",
    "        elif target_model_update >= 1:\n",
    "            # Hard update every `target_model_update` steps.\n",
    "            target_model_update = int(target_model_update)\n",
    "        else:\n",
    "            # Soft update with `(1 - target_model_update) * old + target_model_update * new`.\n",
    "            target_model_update = float(target_model_update)\n",
    "\n",
    "        if delta_range is not None:\n",
    "            warnings.warn('`delta_range` is deprecated. Please use `delta_clip` instead, which takes a single scalar. For now we\\'re falling back to `delta_range[1] = {}`'.format(delta_range[1]))\n",
    "            delta_clip = delta_range[1]\n",
    "\n",
    "        # Parameters.\n",
    "        self.nb_actions = nb_actions\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_steps_warmup = nb_steps_warmup\n",
    "        self.train_interval = train_interval\n",
    "        self.memory_interval = memory_interval\n",
    "        self.target_model_update = target_model_update\n",
    "        self.delta_clip = delta_clip\n",
    "        self.custom_model_objects = custom_model_objects\n",
    "\n",
    "        # Related objects.\n",
    "        self.memory = memory\n",
    "\n",
    "        # State.\n",
    "        self.compiled = False\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        batch = np.array(batch)\n",
    "        if self.processor is None:\n",
    "            return batch\n",
    "        return self.processor.process_state_batch(batch)\n",
    "\n",
    "    def compute_batch_q_values(self, state_batch):\n",
    "        batch = self.process_state_batch(state_batch)\n",
    "        #print(\"batch\",type(batch),batch)\n",
    "        q_values = self.model.predict_on_batch(batch)\n",
    "        assert q_values.shape == (len(state_batch), self.nb_actions)\n",
    "        return q_values\n",
    "\n",
    "    def compute_q_values(self, state):\n",
    "        q_values = self.compute_batch_q_values([state]).flatten()\n",
    "        assert q_values.shape == (self.nb_actions,)\n",
    "        return q_values\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'nb_actions': self.nb_actions,\n",
    "            'gamma': self.gamma,\n",
    "            'batch_size': self.batch_size,\n",
    "            'nb_steps_warmup': self.nb_steps_warmup,\n",
    "            'train_interval': self.train_interval,\n",
    "            'memory_interval': self.memory_interval,\n",
    "            'target_model_update': self.target_model_update,\n",
    "            'delta_clip': self.delta_clip,\n",
    "            'memory': get_object_config(self.memory),\n",
    "        }\n",
    "\n",
    "\n",
    "class DQNAgent(AbstractDQNAgent):\n",
    "    \"\"\"\n",
    "    # Arguments \n",
    "        model__: A Keras model. \n",
    "        policy__: A Keras-rl policy that are defined in [policy](https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py). \n",
    "        test_policy__: A Keras-rl policy. \n",
    "        enable_double_dqn__: A boolean which enable target network as a second network proposed by van Hasselt et al. to decrease overfitting. \n",
    "        enable_dueling_dqn__: A boolean which enable dueling architecture proposed by Mnih et al. \n",
    "        dueling_type__: If `enable_dueling_dqn` is set to `True`, a type of dueling architecture must be chosen which calculate Q(s,a) from V(s) and A(s,a) differently. Note that `avg` is recommanded in the [paper](https://arxiv.org/abs/1511.06581). \n",
    "            `avg`: Q(s,a;theta) = V(s;theta) + (A(s,a;theta)-Avg_a(A(s,a;theta))) \n",
    "            `max`: Q(s,a;theta) = V(s;theta) + (A(s,a;theta)-max_a(A(s,a;theta))) \n",
    "            `naive`: Q(s,a;theta) = V(s;theta) + A(s,a;theta) \n",
    " \n",
    "    \"\"\"\n",
    "    def __init__(self, model, policy=None, test_policy=None, enable_double_dqn = True, enable_dueling_network=False,\n",
    "                 dueling_type='avg', *args, **kwargs):\n",
    "        super(DQNAgent, self).__init__(*args, **kwargs)\n",
    "        '''enable_double_dqn=Falseに変更したら，DQNのbackwardがDDQNじゃなくなる'''\n",
    "\n",
    "        # Validate (important) input.\n",
    "        if hasattr(model.output, '__len__') and len(model.output) > 1:\n",
    "            raise ValueError('Model \"{}\" has more than one output. DQN expects a model that has a single output.'.format(model))\n",
    "        if model.output._keras_shape != (None, self.nb_actions):\n",
    "            raise ValueError('Model output \"{}\" has invalid shape. DQN expects a model that has one dimension for each action, in this case {}.'.format(model.output, self.nb_actions))\n",
    "\n",
    "        # Parameters.\n",
    "        self.enable_double_dqn = enable_double_dqn\n",
    "        self.enable_dueling_network = enable_dueling_network\n",
    "        self.dueling_type = dueling_type\n",
    "        if self.enable_dueling_network:\n",
    "            # get the second last layer of the model, abandon the last layer\n",
    "            layer = model.layers[-2]\n",
    "            nb_action = model.output._keras_shape[-1]\n",
    "            # layer y has a shape (nb_action+1,)\n",
    "            # y[:,0] represents V(s;theta)\n",
    "            # y[:,1:] represents A(s,a;theta)\n",
    "            y = Dense(nb_action + 1, activation='linear')(layer.output)\n",
    "            # caculate the Q(s,a;theta)\n",
    "            # dueling_type == 'avg'\n",
    "            # Q(s,a;theta) = V(s;theta) + (A(s,a;theta)-Avg_a(A(s,a;theta)))\n",
    "            # dueling_type == 'max'\n",
    "            # Q(s,a;theta) = V(s;theta) + (A(s,a;theta)-max_a(A(s,a;theta)))\n",
    "            # dueling_type == 'naive'\n",
    "            # Q(s,a;theta) = V(s;theta) + A(s,a;theta)\n",
    "            if self.dueling_type == 'avg':\n",
    "                outputlayer = Lambda(lambda a: K.expand_dims(a[:, 0], -1) + a[:, 1:] - K.mean(a[:, 1:], keepdims=True), output_shape=(nb_action,))(y)\n",
    "            elif self.dueling_type == 'max':\n",
    "                outputlayer = Lambda(lambda a: K.expand_dims(a[:, 0], -1) + a[:, 1:] - K.max(a[:, 1:], keepdims=True), output_shape=(nb_action,))(y)\n",
    "            elif self.dueling_type == 'naive':\n",
    "                outputlayer = Lambda(lambda a: K.expand_dims(a[:, 0], -1) + a[:, 1:], output_shape=(nb_action,))(y)\n",
    "            else:\n",
    "                assert False, \"dueling_type must be one of {'avg','max','naive'}\"\n",
    "\n",
    "            model = Model(inputs=model.input, outputs=outputlayer)\n",
    "\n",
    "        # Related objects.\n",
    "        self.model = model\n",
    "        if policy is None:\n",
    "            policy = EpsGreedyQPolicy()\n",
    "        if test_policy is None:\n",
    "            test_policy = EpsGreedyQPolicy(eps=0)\n",
    "        self.policy = policy\n",
    "        self.test_policy = test_policy\n",
    "\n",
    "        # State.\n",
    "        self.reset_states()\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(DQNAgent, self).get_config()\n",
    "        config['enable_double_dqn'] = self.enable_double_dqn\n",
    "        config['dueling_type'] = self.dueling_type\n",
    "        config['enable_dueling_network'] = self.enable_dueling_network\n",
    "        config['model'] = get_object_config(self.model)\n",
    "        config['policy'] = get_object_config(self.policy)\n",
    "        config['test_policy'] = get_object_config(self.test_policy)\n",
    "        if self.compiled:\n",
    "            config['target_model'] = get_object_config(self.target_model)\n",
    "        return config\n",
    "\n",
    "    def compile(self, optimizer, metrics=[]):\n",
    "        metrics += [mean_q]  # register default metrics\n",
    "\n",
    "        # We never train the target model, hence we can set the optimizer and loss arbitrarily.\n",
    "        self.target_model = clone_model(self.model, self.custom_model_objects)\n",
    "        self.target_model.compile(optimizer='sgd', loss='mse')\n",
    "        self.model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "        # Compile model.\n",
    "        if self.target_model_update < 1.:\n",
    "            # We use the `AdditionalUpdatesOptimizer` to efficiently soft-update the target model.\n",
    "            updates = get_soft_target_model_updates(self.target_model, self.model, self.target_model_update)\n",
    "            optimizer = AdditionalUpdatesOptimizer(optimizer, updates)\n",
    "\n",
    "        def clipped_masked_error(args):\n",
    "            y_true, y_pred, mask = args\n",
    "            loss = huber_loss(y_true, y_pred, self.delta_clip)\n",
    "            loss *= mask  # apply element-wise mask\n",
    "            return K.sum(loss, axis=-1)\n",
    "\n",
    "        # Create trainable model. The problem is that we need to mask the output since we only\n",
    "        # ever want to update the Q values for a certain action. The way we achieve this is by\n",
    "        # using a custom Lambda layer that computes the loss. This gives us the necessary flexibility\n",
    "        # to mask out certain parameters by passing in multiple inputs to the Lambda layer.\n",
    "        y_pred = self.model.output\n",
    "        y_true = Input(name='y_true', shape=(self.nb_actions,))\n",
    "        mask = Input(name='mask', shape=(self.nb_actions,))\n",
    "        loss_out = Lambda(clipped_masked_error, output_shape=(1,), name='loss')([y_true, y_pred, mask])\n",
    "        ins = [self.model.input] if type(self.model.input) is not list else self.model.input\n",
    "        trainable_model = Model(inputs=ins + [y_true, mask], outputs=[loss_out, y_pred])\n",
    "        assert len(trainable_model.output_names) == 2\n",
    "        combined_metrics = {trainable_model.output_names[1]: metrics}\n",
    "        losses = [\n",
    "            lambda y_true, y_pred: y_pred,  # loss is computed in Lambda layer\n",
    "            lambda y_true, y_pred: K.zeros_like(y_pred),  # we only include this for the metrics\n",
    "        ]\n",
    "        trainable_model.compile(optimizer=optimizer, loss=losses, metrics=combined_metrics)\n",
    "        self.trainable_model = trainable_model\n",
    "\n",
    "        self.compiled = True\n",
    "\n",
    "    def load_weights(self, filepath):\n",
    "        self.model.load_weights(filepath)\n",
    "        self.update_target_model_hard()\n",
    "\n",
    "    def save_weights(self, filepath, overwrite=False):\n",
    "        self.model.save_weights(filepath, overwrite=overwrite)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.recent_action = None\n",
    "        self.recent_observation = None\n",
    "        if self.compiled:\n",
    "            self.model.reset_states()\n",
    "            self.target_model.reset_states()\n",
    "\n",
    "    def update_target_model_hard(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def forward(self, observation,graph):\n",
    "        #print(\"edges\")\n",
    "        #print(graph.edges)\n",
    "        #input()\n",
    "        # Select an action.\n",
    "        state = self.memory.get_recent_state(observation)\n",
    "        #print(\"state\",state)\n",
    "        q_values = self.compute_q_values(state)\n",
    "        #print(\"q_val\",q_values)\n",
    "        if self.training:\n",
    "            #print(\"self.traing\")\n",
    "            action = self.policy.select_action(q_values=q_values,graph=graph)\n",
    "            #print(q_values)\n",
    "        else:\n",
    "            action = self.test_policy.select_action(q_values=q_values,graph=graph)\n",
    "\n",
    "        # Book-keeping.\n",
    "        self.recent_observation = observation\n",
    "        self.recent_action = action\n",
    "\n",
    "        return action\n",
    "\n",
    "    def backward(self,graph, reward, terminal):\n",
    "        # Store most recent experience in memory.\n",
    "        if self.step % self.memory_interval == 0:\n",
    "            self.memory.append(self.recent_observation, self.recent_action, reward, terminal,\n",
    "                               training=self.training)\n",
    "\n",
    "        metrics = [np.nan for _ in self.metrics_names]\n",
    "        if not self.training:\n",
    "            # We're done here. No need to update the experience memory since we only use the working\n",
    "            # memory to obtain the state over the most recent observations.\n",
    "            return metrics\n",
    "\n",
    "        # Train the network on a single stochastic batch.\n",
    "        if self.step > self.nb_steps_warmup and self.step % self.train_interval == 0:\n",
    "            experiences = self.memory.sample(self.batch_size)\n",
    "            assert len(experiences) == self.batch_size\n",
    "\n",
    "            # Start by extracting the necessary parameters (we use a vectorized implementation).\n",
    "            state0_batch = []\n",
    "            reward_batch = []\n",
    "            action_batch = []\n",
    "            terminal1_batch = []\n",
    "            state1_batch = []\n",
    "            for e in experiences:\n",
    "                state0_batch.append(e.state0)\n",
    "                state1_batch.append(e.state1)\n",
    "                reward_batch.append(e.reward)\n",
    "                action_batch.append(e.action)\n",
    "                terminal1_batch.append(0. if e.terminal1 else 1.)\n",
    "\n",
    "            # Prepare and validate parameters.\n",
    "            state0_batch = self.process_state_batch(state0_batch)\n",
    "            state1_batch = self.process_state_batch(state1_batch)\n",
    "            terminal1_batch = np.array(terminal1_batch)\n",
    "            reward_batch = np.array(reward_batch)\n",
    "            assert reward_batch.shape == (self.batch_size,)\n",
    "            assert terminal1_batch.shape == reward_batch.shape\n",
    "            assert len(action_batch) == len(reward_batch)\n",
    "\n",
    "            # Compute Q values for mini-batch update.\n",
    "            if self.enable_double_dqn:\n",
    "                # According to the paper \"Deep Reinforcement Learning with Double Q-learning\"\n",
    "                # (van Hasselt et al., 2015), in Double DQN, the online network predicts the actions\n",
    "                # while the target network is used to estimate the Q value.\n",
    "                q_values = self.model.predict_on_batch(state1_batch)\n",
    "                assert q_values.shape == (self.batch_size, self.nb_actions)\n",
    "                actions = np.argmax(q_values, axis=1)\n",
    "                assert actions.shape == (self.batch_size,)\n",
    "\n",
    "                # Now, estimate Q values using the target network but select the values with the\n",
    "                # highest Q value wrt to the online model (as computed above).\n",
    "                target_q_values = self.target_model.predict_on_batch(state1_batch)\n",
    "                assert target_q_values.shape == (self.batch_size, self.nb_actions)\n",
    "                q_batch = target_q_values[range(self.batch_size), actions]\n",
    "            else:\n",
    "                # Compute the q_values given state1, and extract the maximum for each sample in the batch.\n",
    "                # We perform this prediction on the target_model instead of the model for reasons\n",
    "                # outlined in Mnih (2015). In short: it makes the algorithm more stable.\n",
    "                target_q_values = self.target_model.predict_on_batch(state1_batch)\n",
    "                assert target_q_values.shape == (self.batch_size, self.nb_actions)\n",
    "                \"\"\"多分ココや！\"\"\"\n",
    "                print(target_q_values.shape)\n",
    "                #ここでfilter_nodeかける。\n",
    "                v_num = len(graph.nodes)\n",
    "                node_filter = readed.filter_node(graph,v_num)\n",
    "                #print(\"len:qval\",len(q_values))\n",
    "                #print(\"len:filter\",len(node_filter))\n",
    "                MINUS_INF = -999999\n",
    "                \"\"\"\n",
    "                masked_target_q_values = copy(target_q_values)\n",
    "                for i in range(len(q_values)):\n",
    "                    if(node_filter[i]!=1):\n",
    "                        masked_target_q_values[i]= MINUS_INF\n",
    "                \"\"\"\n",
    "                q_batch = np.max(target_q_values, axis=1).flatten()\n",
    "                print(\"q_batch\",q_batch)\n",
    "                input()\n",
    "            assert q_batch.shape == (self.batch_size,)\n",
    "            targets = np.zeros((self.batch_size, self.nb_actions))\n",
    "            dummy_targets = np.zeros((self.batch_size,))\n",
    "            masks = np.zeros((self.batch_size, self.nb_actions))\n",
    "\n",
    "            # Compute r_t + gamma * max_a Q(s_t+1, a) and update the target targets accordingly,\n",
    "            # but only for the affected output units (as given by action_batch).\n",
    "            discounted_reward_batch = self.gamma * q_batch\n",
    "            # Set discounted reward to zero for all states that were terminal.\n",
    "            discounted_reward_batch *= terminal1_batch\n",
    "            assert discounted_reward_batch.shape == reward_batch.shape\n",
    "            Rs = reward_batch + discounted_reward_batch\n",
    "            for idx, (target, mask, R, action) in enumerate(zip(targets, masks, Rs, action_batch)):\n",
    "                target[action] = R  # update action with estimated accumulated reward\n",
    "                dummy_targets[idx] = R\n",
    "                mask[action] = 1.  # enable loss for this specific action\n",
    "            targets = np.array(targets).astype('float32')\n",
    "            masks = np.array(masks).astype('float32')\n",
    "\n",
    "            # Finally, perform a single update on the entire batch. We use a dummy target since\n",
    "            # the actual loss is computed in a Lambda layer that needs more complex input. However,\n",
    "            # it is still useful to know the actual target to compute metrics properly.\n",
    "            ins = [state0_batch] if type(self.model.input) is not list else state0_batch\n",
    "            metrics = self.trainable_model.train_on_batch(ins + [targets, masks], [dummy_targets, targets])\n",
    "            metrics = [metric for idx, metric in enumerate(metrics) if idx not in (1, 2)]  # throw away individual losses\n",
    "            metrics += self.policy.metrics\n",
    "            if self.processor is not None:\n",
    "                metrics += self.processor.metrics\n",
    "\n",
    "        if self.target_model_update >= 1 and self.step % self.target_model_update == 0:\n",
    "            self.update_target_model_hard()\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return self.model.layers[:]\n",
    "\n",
    "    @property\n",
    "    def metrics_names(self):\n",
    "        # Throw away individual losses and replace output name since this is hidden from the user.\n",
    "        assert len(self.trainable_model.output_names) == 2\n",
    "        dummy_output_name = self.trainable_model.output_names[1]\n",
    "        model_metrics = [name for idx, name in enumerate(self.trainable_model.metrics_names) if idx not in (1, 2)]\n",
    "        model_metrics = [name.replace(dummy_output_name + '_', '') for name in model_metrics]\n",
    "\n",
    "        names = model_metrics + self.policy.metrics_names[:]\n",
    "        if self.processor is not None:\n",
    "            names += self.processor.metrics_names[:]\n",
    "        return names\n",
    "\n",
    "    @property\n",
    "    def policy(self):\n",
    "        return self.__policy\n",
    "\n",
    "    @policy.setter\n",
    "    def policy(self, policy):\n",
    "        self.__policy = policy\n",
    "        self.__policy._set_agent(self)\n",
    "\n",
    "    @property\n",
    "    def test_policy(self):\n",
    "        return self.__test_policy\n",
    "\n",
    "    @test_policy.setter\n",
    "    def test_policy(self, policy):\n",
    "        self.__test_policy = policy\n",
    "        self.__test_policy._set_agent(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graph_control_0709'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-bd2b559e5da8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgraph_control_0709\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graph_control_0709'"
     ]
    }
   ],
   "source": [
    "#0501から大改革！\n",
    "\n",
    "import gym\n",
    "import gym.spaces\n",
    "import graph_control_0709 as readed\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "#graph_set = readed.Graph()\n",
    "#graph_list = readed.convert_graph_into_NN_input(graph_set)\n",
    "#print(graph_list)\n",
    "#num = len(graph_list)\n",
    "#print(num)\n",
    "#print(\"vertex_number\",vertex_number)\n",
    "\n",
    "outbound_station_list = ['shibuya','sakura-shimmachi','kajigaya','saginuma','eda','nagatsuta','chuo-rinkan']\n",
    "inbound_station_list = ['chuo-rinkan','nagatsuta','fujigaoka','eda','saginuma','kajigaya','sakura-shimmmachi','shibuya']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "07/15 小原\n",
    "\n",
    "OPENGYMは以下のclass Environmentにて使用しています．\n",
    "inputはグラフの各辺の重み情報＋プラットフォーム情報の1次元配列(self.graph_list)\n",
    "outputも同様に，グラフ＋プラットフォームに関する行えうるすべての操作を1次元配列にしたものです(1次元配列のindex -> operation)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Environment(gym.core.Env):\n",
    "    def __init__(self):\n",
    "        self.graph_set = readed.Graph()\n",
    "        self.platform_number = 2\n",
    "        self.vertex_number = len(list(self.graph_set.graph.nodes))\n",
    "        self.graph_list = readed.convert_graph_into_NN_input(self.graph_set)\n",
    "        self.output_num = self.vertex_number**2 + self.vertex_number*self.platform_number\n",
    "        self.input_num = len(self.graph_list)\n",
    "        #print(self.input_num)\n",
    "        #print('graph_list',self.graph_list)\n",
    "        #print(self.num)\n",
    "        self.action_space = gym.spaces.Discrete(self.output_num)\n",
    "        high = np.ones(self.input_num) \n",
    "        low = np.zeros(self.input_num)        \n",
    "        self.observation_space = gym.spaces.Box(low=low, high=high)\n",
    "        \n",
    "    def step(self, action):\n",
    "        #print('action',action)\n",
    "        action_info = readed.convert_index_into_operation(action,self.vertex_number)\n",
    "        #print('action_info',action_info)\n",
    "        score_instance = readed.Score_grading()\n",
    "        pre_score = score_instance.discontent_grading(self.graph_set.graph)\n",
    "        #print('pre_score:',pre_score)\n",
    "        #NOW からupdateがきまる\n",
    "        self.graph_set.graph = self.graph_set.remodeling_graph(action_info)\n",
    "        self._state = readed.convert_graph_into_NN_input(self.graph_set)\n",
    "        score_instance = readed.Score_grading()\n",
    "        score = score_instance.discontent_grading(self.graph_set.graph)\n",
    "        #print(\"score\",score)\n",
    "        #print(\"pre_score\",pre_score)\n",
    "        step_score = score - pre_score\n",
    "        #print(\"step_score\",step_score)\n",
    "        #print('score',score)\n",
    "        #print(\"step_score\",step_score)\n",
    "        #game 終了判定\n",
    "        done = action == 0\n",
    "        \n",
    "        if done:\n",
    "            reward = 0    #TODO:考えなおす=>これでええやろ\n",
    "        else:\n",
    "            reward = step_score\n",
    "            if(reward>0):\n",
    "                reward  = reward\n",
    "            else:\n",
    "                reward = (reward-1)\n",
    "                #print(\"Bad Choice\")\n",
    "                done = True\n",
    "            #良い操作に対してはめっちゃ褒めるようにした\n",
    "            #悪い操作したら強制終了\n",
    "        \n",
    "        return np.array(self._state), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.graph_set = readed.Graph()\n",
    "        #self.platform_number = 2\n",
    "        #self.vertex_number = len(list(self.graph_set.graph.nodes))\n",
    "        self.graph_list = readed.convert_graph_into_NN_input(self.graph_set)\n",
    "        #self.output_num = self.vertex_number**2 + self.vertex_number*self.platform_number\n",
    "        #self.input_num = len(self.graph_list)\n",
    "        ##print(self.input_num)\n",
    "        ##print('graph_list',self.graph_list)\n",
    "        ##print(self.num)\n",
    "        #self.action_space = gym.spaces.Discrete(self.output_num)\n",
    "        #high = np.ones(self.input_num) # max valueを求める? later\n",
    "        #low = np.zeros(self.input_num)        \n",
    "        #self.observation_space = gym.spaces.Box(low=low, high=high)\n",
    "        self._state = np.array(self.graph_list)\n",
    "        return np.array(self._state)\n",
    "    \n",
    "    def generate_randomized_accident(self):\n",
    "        global outbound_station_list\n",
    "        global inbound_station_list\n",
    "        #accident = [station_a,station_b,bound,delay_start_time,delay_period]\n",
    "        bound = random.choice(['in','out'])\n",
    "        if(bound == 'in'):\n",
    "            station_list = inbound_station_list\n",
    "        elif(bound == 'out'):\n",
    "            station_list = outbound_station_list\n",
    "            \n",
    "        list_size = len(station_list)\n",
    "        station_a_number = random.choice(range(list_size))\n",
    "        \n",
    "        if(station_a_number == list_size - 1):\n",
    "            number = 0\n",
    "        else:\n",
    "            number = random.choice([0,1])\n",
    "        \n",
    "        station_b_number = station_a_number + number\n",
    "        station_a = station_list[station_a_number]\n",
    "        station_b = station_list[station_b_number]\n",
    "        delay_start_time = random.choice(range(readed.edge_min_time,readed.edge_max_time))\n",
    "        delay_period = random.choice([180,360,720,900])\n",
    "        return [station_a,station_b,bound,delay_start_time,delay_period] #accident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rl.callbacks\n",
    "class EpisodeLogger(rl.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.observations = {}\n",
    "        self.rewards = {}\n",
    "        self.actions = {}\n",
    "\n",
    "    def on_episode_begin(self, episode, logs):\n",
    "        self.observations[episode] = []\n",
    "        self.rewards[episode] = []\n",
    "        self.actions[episode] = []\n",
    "\n",
    "    def on_step_end(self, step, logs):\n",
    "        episode = logs['episode']\n",
    "        self.observations[episode].append(logs['observation'])\n",
    "        self.rewards[episode].append(logs['reward'])\n",
    "        self.actions[episode].append(logs['action'])\n",
    "\n",
    "\n",
    "'''\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for obs in cb_ep.observations.values():\n",
    "    plt.plot([o[0] for o in obs])\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"pos\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "env = Environment()\n",
    "nb_actions = env.action_space.n #nこの操作がある\n",
    "#print(nb_actions)\n",
    "#print(env.observation_space.shape)\n",
    "# DQNのネットワーク定義\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(2500))\n",
    "#model.add(Dense(2500, init ='zero'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2500))\n",
    "#model.add(Dense(2500, init ='zero'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2500))\n",
    "#model.add(Dense(2500, init ='zero'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "#print(model.summary())\n",
    "\n",
    "# experience replay用のmemory\n",
    "memory = SequentialMemory(limit=50000, window_length=1) \n",
    "# 行動方策はオーソドックスなepsilon-greedy。ほかに、各行動のQ値によって確率を決定するBoltzmannQPolicyが利用可能\n",
    "policy = EpsGreedyQPolicy(eps=0.01) \n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-3, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-4), metrics=['mae'])\n",
    "repeat_set = 50\n",
    "#dqn.save_weights('./model-0508',True)\n",
    "#dqn.save_weights('./model-0614',True)  ## NOTICE**!!!! initialized NN\n",
    "dqn.load_weights('./model-0614ver11')\n",
    "#mode = \"train\"\n",
    "mode = \"test\"\n",
    "if(mode==\"train\"):\n",
    "    for repeat in range(repeat_set):\n",
    "        env = Environment()\n",
    "        #accident = env.generate_randomized_accident()\n",
    "\n",
    "        #accident=['shibuya','shibuya','out',27020,240]\n",
    "        accident = env.generate_randomized_accident()\n",
    "        print(\"repeat_time\",repeat ,\"/\",repeat_set)\n",
    "        print(\"accident_info\",accident)\n",
    "        accident =  ['chuo-rinkan', 'nagatsuta', 'in', 32680, 180]\n",
    "        \n",
    "        env.graph_set.graph = env.graph_set.input_delay(accident[0],accident[1],accident[2],accident[3],accident[4])\n",
    "        #accident = [station_a,station_b,bound,delay_start_time,delay_period]\n",
    "        history = dqn.fit(env, nb_steps=2000,graph = env.graph_set.graph ,visualize=False, verbose=2, nb_max_episode_steps=10) #TODO:Reconsider hyper param\n",
    "        #DAGじゃない時の対処？→スタート点の自己ループの気がする。多分遅延を入れるときに、自己ループを産んでしまうとかかな？？？\n",
    "        #あとなんかたまにtupleのエラー出る\n",
    "        #print('ok')\n",
    "        #nb_step = step*episode\n",
    "\n",
    "    dqn.save_weights('./model-0614ver12',True)\n",
    "\n",
    "    #ver 1 :10000\n",
    "    #ver2 :+90000\n",
    "    #ver3 :+100000 eps=0.02\n",
    "    #ver4 :+100000 eps = 0.05\n",
    "    #ver5 :+100000 eps = 0.05\n",
    "    #ver6 :+10000 eps = 0.03\n",
    "    #ver7 : +100000 eps = 0.01\n",
    "    #ver8: +200eps*2000step eps = 0.01\n",
    "    #ver8: +200eps*2000step eps = 0.01\n",
    "    #ver9: +200eps*2000step eps = 0.01\n",
    "    #ver10->11: +200eps*2000step eps = 0.01\n",
    "    #ver11->12: +50eps*2000step eps = 0.01\n",
    "\n",
    "    #latest = ver12\n",
    "else:\n",
    "    for i in range(1000):\n",
    "        env = Environment()\n",
    "        cb_ep = EpisodeLogger()\n",
    "        accident = env.generate_randomized_accident()\n",
    "        #accident=['shibuya','shibuya','out',27020,240]\n",
    "        env.graph_set.graph = env.graph_set.input_delay(accident[0],accident[1],accident[2],accident[3],accident[4])\n",
    "        main_score_instance = readed.Score_grading()\n",
    "        init_score = main_score_instance.discontent_grading(env.graph_set.graph)\n",
    "        if(init_score== -225.15):\n",
    "            continue\n",
    "        print(init_score)\n",
    "        print(\"accident_info\",accident)\n",
    "        dqn.test(env, graph = env.graph_set.graph , nb_episodes=1, visualize=False, callbacks=[cb_ep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "for repeat in range(repeat_set):\n",
    "    env = Environment()\n",
    "    #accident = env.generate_randomized_accident()\n",
    "    accident=['shibuya','shibuya','out',27020,240]\n",
    "    print(accident)\n",
    "    env.graph_set.graph = env.graph_set.input_delay(accident[0],accident[1],accident[2],accident[3],accident[4])\n",
    "    #accident = [station_a,station_b,bound,delay_start_time,delay_period]\n",
    "    history = dqn.fit(env, nb_steps=30000,graph = env.graph_set.graph ,visualize=False, verbose=2, nb_max_episode_steps=10) #TODO:Reconsider hyper param\n",
    "    #DAGじゃない時の対処？→スタート点の自己ループの気がする。多分遅延を入れるときに、自己ループを産んでしまうとかかな？？？\n",
    "    #あとなんかたまにtupleのエラー出る\n",
    "    #print('ok')\n",
    "    #nb_step = step*episode\n",
    "    \n",
    "dqn.save_weights('./model-0614',True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
